{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "740b4884",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-25T15:38:32.490929Z",
     "iopub.status.busy": "2024-11-25T15:38:32.490551Z",
     "iopub.status.idle": "2024-11-25T15:38:35.600523Z",
     "shell.execute_reply": "2024-11-25T15:38:35.599734Z"
    },
    "papermill": {
     "duration": 3.11714,
     "end_time": "2024-11-25T15:38:35.602551",
     "exception": false,
     "start_time": "2024-11-25T15:38:32.485411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import librosa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "586cf22b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T15:38:35.609991Z",
     "iopub.status.busy": "2024-11-25T15:38:35.609617Z",
     "iopub.status.idle": "2024-11-25T15:38:45.478982Z",
     "shell.execute_reply": "2024-11-25T15:38:45.477786Z"
    },
    "papermill": {
     "duration": 9.874933,
     "end_time": "2024-11-25T15:38:45.480781",
     "exception": false,
     "start_time": "2024-11-25T15:38:35.605848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pystoi\r\n",
      "  Downloading pystoi-0.4.1-py2.py3-none-any.whl.metadata (4.0 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pystoi) (1.26.4)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from pystoi) (1.14.1)\r\n",
      "Downloading pystoi-0.4.1-py2.py3-none-any.whl (8.2 kB)\r\n",
      "Installing collected packages: pystoi\r\n",
      "Successfully installed pystoi-0.4.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pystoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ee1572",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T15:38:45.488769Z",
     "iopub.status.busy": "2024-11-25T15:38:45.488444Z",
     "iopub.status.idle": "2024-11-25T15:39:02.345711Z",
     "shell.execute_reply": "2024-11-25T15:39:02.344668Z"
    },
    "papermill": {
     "duration": 16.863773,
     "end_time": "2024-11-25T15:39:02.347869",
     "exception": false,
     "start_time": "2024-11-25T15:38:45.484096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pesq\r\n",
      "  Downloading pesq-0.0.4.tar.gz (38 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hBuilding wheels for collected packages: pesq\r\n",
      "  Building wheel for pesq (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pesq: filename=pesq-0.0.4-cp310-cp310-linux_x86_64.whl size=114934 sha256=7f5851e7e8c883711f4a7baba2a89327f0ed86ea4f3ef526c126d3ffe97026df\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/c5/4e/2c/251524370c0fdd659e99639a0fbd0ca5a782c3aafcd456b28d\r\n",
      "Successfully built pesq\r\n",
      "Installing collected packages: pesq\r\n",
      "Successfully installed pesq-0.0.4\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pesq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "091f38d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T15:39:02.357347Z",
     "iopub.status.busy": "2024-11-25T15:39:02.357009Z",
     "iopub.status.idle": "2024-11-25T15:39:02.367456Z",
     "shell.execute_reply": "2024-11-25T15:39:02.366772Z"
    },
    "papermill": {
     "duration": 0.017134,
     "end_time": "2024-11-25T15:39:02.369130",
     "exception": false,
     "start_time": "2024-11-25T15:39:02.351996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, clean_dir, noisy_dir, sample_rate=16000, segment_length=16000):\n",
    "        super(SpeechDataset, self).__init__()\n",
    "        self.clean_dir = clean_dir\n",
    "        self.noisy_dir = noisy_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.segment_length = segment_length\n",
    "\n",
    "        self.clean_files = sorted([\n",
    "            f for f in os.listdir(clean_dir)\n",
    "            if os.path.isfile(os.path.join(clean_dir, f))\n",
    "        ])\n",
    "        self.noisy_files = sorted([\n",
    "            f for f in os.listdir(noisy_dir)\n",
    "            if os.path.isfile(os.path.join(noisy_dir, f))\n",
    "        ])\n",
    "\n",
    "        assert len(self.clean_files) == len(self.noisy_files), \\\n",
    "            f\"Number of clean files ({len(self.clean_files)}) does not match number of noisy files ({len(self.noisy_files)}).\"\n",
    "\n",
    "        for clean_file, noisy_file in zip(self.clean_files, self.noisy_files):\n",
    "            assert clean_file == noisy_file, \\\n",
    "                f\"Filename mismatch: {clean_file} and {noisy_file}\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clean_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.clean_files[idx]\n",
    "        clean_path = os.path.join(self.clean_dir, filename)\n",
    "        noisy_path = os.path.join(self.noisy_dir, filename)\n",
    "\n",
    "        clean, _ = librosa.load(clean_path, sr=self.sample_rate)\n",
    "        noisy, _ = librosa.load(noisy_path, sr=self.sample_rate)\n",
    "\n",
    "        min_len = min(len(clean), len(noisy))\n",
    "        clean = clean[:min_len]\n",
    "        noisy = noisy[:min_len]\n",
    "\n",
    "        if len(clean) < self.segment_length:\n",
    "            pad_width = self.segment_length - len(clean)\n",
    "            clean = np.pad(clean, (0, pad_width), 'constant')\n",
    "            noisy = np.pad(noisy, (0, pad_width), 'constant')\n",
    "        else:\n",
    "            start = np.random.randint(0, len(clean) - self.segment_length + 1)\n",
    "            clean = clean[start:start + self.segment_length]\n",
    "            noisy = noisy[start:start + self.segment_length]\n",
    "\n",
    "        clean = torch.from_numpy(clean).float().unsqueeze(0)\n",
    "        noisy = torch.from_numpy(noisy).float().unsqueeze(0)\n",
    "\n",
    "        return noisy, clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0809ef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T15:39:02.379441Z",
     "iopub.status.busy": "2024-11-25T15:39:02.379128Z",
     "iopub.status.idle": "2024-11-25T15:39:02.383023Z",
     "shell.execute_reply": "2024-11-25T15:39:02.382283Z"
    },
    "papermill": {
     "duration": 0.011208,
     "end_time": "2024-11-25T15:39:02.384616",
     "exception": false,
     "start_time": "2024-11-25T15:39:02.373408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_train_dir = '/kaggle/input/voicebank-demand-16k/clean_trainset_28spk_wav'\n",
    "noisy_train_dir = '/kaggle/input/voicebank-demand-16k/noisy_trainset_28spk_wav'\n",
    "clean_test_dir = '/kaggle/input/voicebank-demand-16k/clean_testset_wav'\n",
    "noisy_test_dir = '/kaggle/input/voicebank-demand-16k/noisy_testset_wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8990e74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T15:39:02.393497Z",
     "iopub.status.busy": "2024-11-25T15:39:02.393170Z",
     "iopub.status.idle": "2024-11-25T15:39:28.117686Z",
     "shell.execute_reply": "2024-11-25T15:39:28.116938Z"
    },
    "papermill": {
     "duration": 25.731648,
     "end_time": "2024-11-25T15:39:28.120111",
     "exception": false,
     "start_time": "2024-11-25T15:39:02.388463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = SpeechDataset(clean_dir=clean_train_dir, noisy_dir=noisy_train_dir)\n",
    "test_dataset = SpeechDataset(clean_dir=clean_test_dir, noisy_dir=noisy_test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29e1ecff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T15:39:28.129553Z",
     "iopub.status.busy": "2024-11-25T15:39:28.129204Z",
     "iopub.status.idle": "2024-11-25T15:39:28.157164Z",
     "shell.execute_reply": "2024-11-25T15:39:28.156483Z"
    },
    "papermill": {
     "duration": 0.034853,
     "end_time": "2024-11-25T15:39:28.159174",
     "exception": false,
     "start_time": "2024-11-25T15:39:28.124321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "val_size = int(0.5 * len(test_dataset)) \n",
    "test_size = len(test_dataset) - val_size\n",
    "val_dataset, test_dataset = random_split(test_dataset, [val_size, test_size])\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03a0d63d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T15:39:28.168583Z",
     "iopub.status.busy": "2024-11-25T15:39:28.168197Z",
     "iopub.status.idle": "2024-11-25T15:39:28.180836Z",
     "shell.execute_reply": "2024-11-25T15:39:28.180153Z"
    },
    "papermill": {
     "duration": 0.019259,
     "end_time": "2024-11-25T15:39:28.182585",
     "exception": false,
     "start_time": "2024-11-25T15:39:28.163326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_channels=1, output_channels=1, feature_maps=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv1d(noise_channels, feature_maps, kernel_size=15, stride=1, padding=7),\n",
    "            nn.BatchNorm1d(feature_maps),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv1d(feature_maps, feature_maps*2, kernel_size=15, stride=2, padding=7),\n",
    "            nn.BatchNorm1d(feature_maps*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Conv1d(feature_maps*2, feature_maps*4, kernel_size=15, stride=2, padding=7),\n",
    "            nn.BatchNorm1d(feature_maps*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.encoder4 = nn.Sequential(\n",
    "            nn.Conv1d(feature_maps*4, feature_maps*8, kernel_size=15, stride=2, padding=7),\n",
    "            nn.BatchNorm1d(feature_maps*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(feature_maps*8, feature_maps*4, kernel_size=15, stride=2, padding=7, output_padding=1),\n",
    "            nn.BatchNorm1d(feature_maps*4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(feature_maps*8, feature_maps*2, kernel_size=15, stride=2, padding=7, output_padding=1),\n",
    "            nn.BatchNorm1d(feature_maps*2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(feature_maps*4, feature_maps, kernel_size=15, stride=2, padding=7, output_padding=1),\n",
    "            nn.BatchNorm1d(feature_maps),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.decoder4 = nn.Sequential(\n",
    "            nn.Conv1d(feature_maps*2, output_channels, kernel_size=15, stride=1, padding=7),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self.weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)  \n",
    "        enc2 = self.encoder2(enc1)  \n",
    "        enc3 = self.encoder3(enc2)  \n",
    "        enc4 = self.encoder4(enc3)  \n",
    "\n",
    "        dec1 = self.decoder1(enc4)  \n",
    "        dec1 = torch.cat((dec1, enc3), dim=1)  \n",
    "\n",
    "        dec2 = self.decoder2(dec1)  \n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)  \n",
    "\n",
    "        dec3 = self.decoder3(dec2)  \n",
    "        dec3 = torch.cat((dec3, enc1), dim=1)  \n",
    "\n",
    "        dec4 = self.decoder4(dec3) \n",
    "\n",
    "        return dec4\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Conv1d) or isinstance(m, nn.ConvTranspose1d):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias.data, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84194f47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T15:39:28.191743Z",
     "iopub.status.busy": "2024-11-25T15:39:28.191455Z",
     "iopub.status.idle": "2024-11-25T15:39:28.200034Z",
     "shell.execute_reply": "2024-11-25T15:39:28.199241Z"
    },
    "papermill": {
     "duration": 0.015086,
     "end_time": "2024-11-25T15:39:28.201724",
     "exception": false,
     "start_time": "2024-11-25T15:39:28.186638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels=1, feature_maps=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, feature_maps, kernel_size=15, stride=1, padding=7),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv1d(feature_maps, feature_maps*2, kernel_size=15, stride=2, padding=7),\n",
    "            nn.BatchNorm1d(feature_maps*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv1d(feature_maps*2, feature_maps*4, kernel_size=15, stride=2, padding=7),\n",
    "            nn.BatchNorm1d(feature_maps*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv1d(feature_maps*4, feature_maps*8, kernel_size=15, stride=2, padding=7),\n",
    "            nn.BatchNorm1d(feature_maps*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv1d(feature_maps*8, 1, kernel_size=15, stride=1, padding=7),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self.weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).view(-1, 1)  # Flatten output\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        \"\"\"Initializes weights with a normal distribution.\"\"\"\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0)\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias.data, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4f18cd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T15:39:28.210542Z",
     "iopub.status.busy": "2024-11-25T15:39:28.210251Z",
     "iopub.status.idle": "2024-11-25T15:39:28.271310Z",
     "shell.execute_reply": "2024-11-25T15:39:28.270406Z"
    },
    "papermill": {
     "duration": 0.067417,
     "end_time": "2024-11-25T15:39:28.273065",
     "exception": false,
     "start_time": "2024-11-25T15:39:28.205648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff5406fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T15:39:28.282367Z",
     "iopub.status.busy": "2024-11-25T15:39:28.282012Z",
     "iopub.status.idle": "2024-11-25T15:39:29.862915Z",
     "shell.execute_reply": "2024-11-25T15:39:29.861966Z"
    },
    "papermill": {
     "duration": 1.587849,
     "end_time": "2024-11-25T15:39:29.864938",
     "exception": false,
     "start_time": "2024-11-25T15:39:28.277089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "adversarial_loss = nn.BCELoss().to(device)\n",
    "l1_loss = nn.L1Loss().to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "lambda_l1 = 100  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52290b0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T15:39:29.874443Z",
     "iopub.status.busy": "2024-11-25T15:39:29.874041Z",
     "iopub.status.idle": "2024-11-25T15:39:30.818737Z",
     "shell.execute_reply": "2024-11-25T15:39:30.818025Z"
    },
    "papermill": {
     "duration": 0.951671,
     "end_time": "2024-11-25T15:39:30.821074",
     "exception": false,
     "start_time": "2024-11-25T15:39:29.869403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pystoi import stoi\n",
    "from pesq import pesq\n",
    "\n",
    "def denormalize_audio(tensor):\n",
    "    return tensor.cpu().numpy()\n",
    "\n",
    "def calculate_snr(clean, denoised):\n",
    "    \n",
    "    noise = clean - denoised\n",
    "    signal_power = np.sum(clean ** 2)\n",
    "    noise_power = np.sum(noise ** 2)\n",
    "    snr = 10 * np.log10(signal_power / noise_power)\n",
    "    return snr\n",
    "\n",
    "def evaluate(generator, val_loader):\n",
    "    generator.eval()\n",
    "    stoi_scores = []\n",
    "    pesq_scores = []\n",
    "    snr_scores = []\n",
    "    with torch.no_grad():\n",
    "        for noisy, clean in val_loader:\n",
    "            noisy = noisy.to(device)\n",
    "            clean = clean.to(device)\n",
    "\n",
    "            denoised = generator(noisy)\n",
    "\n",
    "            clean_np = denormalize_audio(clean.squeeze(1))  \n",
    "            denoised_np = denormalize_audio(denoised.squeeze(1))  \n",
    "\n",
    "            for c, d in zip(clean_np, denoised_np):\n",
    "                if len(c) < 256 or len(d) < 256:\n",
    "                    print(\"Skipping short audio segment.\")\n",
    "                    continue\n",
    "                if np.allclose(c, 0) or np.allclose(d, 0):\n",
    "                    print(\"Skipping silent audio segment.\")\n",
    "                    continue\n",
    "                c = np.clip(c / np.max(np.abs(c)), -1, 1)\n",
    "                d = np.clip(d / np.max(np.abs(d)), -1, 1)\n",
    "\n",
    "                stoi_score = stoi(c, d, 16000) \n",
    "                stoi_scores.append(stoi_score)\n",
    "\n",
    "                try:\n",
    "                    pesq_score = pesq(16000, c, d, mode=\"wb\")  \n",
    "                    pesq_scores.append(pesq_score)\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "                snr_score = calculate_snr(c, d)\n",
    "                snr_scores.append(snr_score)\n",
    "\n",
    "    avg_stoi = np.mean(stoi_scores) if stoi_scores else 0\n",
    "    avg_pesq = np.mean(pesq_scores) if pesq_scores else 0\n",
    "    avg_snr = np.mean(snr_scores) if snr_scores else 0\n",
    "    return avg_stoi, avg_pesq, avg_snr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be5cb8e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T15:39:30.830848Z",
     "iopub.status.busy": "2024-11-25T15:39:30.830443Z",
     "iopub.status.idle": "2024-11-25T16:59:38.729272Z",
     "shell.execute_reply": "2024-11-25T16:59:38.728378Z"
    },
    "papermill": {
     "duration": 4807.913179,
     "end_time": "2024-11-25T16:59:38.738553",
     "exception": false,
     "start_time": "2024-11-25T15:39:30.825374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Batch [0/724] Loss D: 0.8345, Loss G: 39.4324\n",
      "Epoch [1/10] Batch [100/724] Loss D: 0.1982, Loss G: 8.3693\n",
      "Epoch [1/10] Batch [200/724] Loss D: 0.0401, Loss G: 8.6982\n",
      "Epoch [1/10] Batch [300/724] Loss D: 0.0209, Loss G: 9.5977\n",
      "Epoch [1/10] Batch [400/724] Loss D: 0.0423, Loss G: 6.4585\n",
      "Epoch [1/10] Batch [500/724] Loss D: 0.0805, Loss G: 8.2935\n",
      "Epoch [1/10] Batch [600/724] Loss D: 0.1237, Loss G: 9.7762\n",
      "Epoch [1/10] Batch [700/724] Loss D: 0.0064, Loss G: 8.8324\n",
      "Epoch [1/10] Validation - STOI: 0.7771, PESQ: 1.0509, SNR: -0.2734\n",
      "Epoch [1/10] completed.\n",
      "Epoch [2/10] Batch [0/724] Loss D: 1.4371, Loss G: 10.4038\n",
      "Epoch [2/10] Batch [100/724] Loss D: 0.0098, Loss G: 9.3344\n",
      "Epoch [2/10] Batch [200/724] Loss D: 0.0055, Loss G: 11.3623\n",
      "Epoch [2/10] Batch [300/724] Loss D: 0.0220, Loss G: 9.1178\n",
      "Epoch [2/10] Batch [400/724] Loss D: 0.0053, Loss G: 9.0103\n",
      "Epoch [2/10] Batch [500/724] Loss D: 0.4394, Loss G: 9.2351\n",
      "Epoch [2/10] Batch [600/724] Loss D: 0.0162, Loss G: 9.9856\n",
      "Epoch [2/10] Batch [700/724] Loss D: 0.0022, Loss G: 9.1440\n",
      "Epoch [2/10] Validation - STOI: 0.8821, PESQ: 1.1752, SNR: 2.3019\n",
      "Saved model checkpoints for epoch 2.\n",
      "Epoch [2/10] completed.\n",
      "Epoch [3/10] Batch [0/724] Loss D: 0.0017, Loss G: 10.2842\n",
      "Epoch [3/10] Batch [100/724] Loss D: 0.0018, Loss G: 10.6755\n",
      "Epoch [3/10] Batch [200/724] Loss D: 0.0137, Loss G: 7.0475\n",
      "Epoch [3/10] Batch [300/724] Loss D: 0.0069, Loss G: 8.3945\n",
      "Epoch [3/10] Batch [400/724] Loss D: 0.0037, Loss G: 9.3512\n",
      "Epoch [3/10] Batch [500/724] Loss D: 0.0156, Loss G: 7.2550\n",
      "Epoch [3/10] Batch [600/724] Loss D: 0.0012, Loss G: 9.2906\n",
      "Epoch [3/10] Batch [700/724] Loss D: 0.0019, Loss G: 10.0650\n",
      "Epoch [3/10] Validation - STOI: 0.8762, PESQ: 1.3239, SNR: 3.9095\n",
      "Epoch [3/10] completed.\n",
      "Epoch [4/10] Batch [0/724] Loss D: 0.0024, Loss G: 11.0692\n",
      "Epoch [4/10] Batch [100/724] Loss D: 0.0013, Loss G: 9.8806\n",
      "Epoch [4/10] Batch [200/724] Loss D: 0.0017, Loss G: 10.5455\n",
      "Epoch [4/10] Batch [300/724] Loss D: 0.0002, Loss G: 12.4282\n",
      "Epoch [4/10] Batch [400/724] Loss D: 0.0009, Loss G: 11.8316\n",
      "Epoch [4/10] Batch [500/724] Loss D: 0.0016, Loss G: 8.8519\n",
      "Epoch [4/10] Batch [600/724] Loss D: 0.0004, Loss G: 11.8154\n",
      "Epoch [4/10] Batch [700/724] Loss D: 0.0022, Loss G: 10.9662\n",
      "Epoch [4/10] Validation - STOI: 0.8859, PESQ: 1.2979, SNR: 5.7587\n",
      "Saved model checkpoints for epoch 4.\n",
      "Epoch [4/10] completed.\n",
      "Epoch [5/10] Batch [0/724] Loss D: 0.0051, Loss G: 11.9374\n",
      "Epoch [5/10] Batch [100/724] Loss D: 0.0053, Loss G: 8.6372\n",
      "Epoch [5/10] Batch [200/724] Loss D: 0.0020, Loss G: 8.4083\n",
      "Epoch [5/10] Batch [300/724] Loss D: 0.0012, Loss G: 9.6549\n",
      "Epoch [5/10] Batch [400/724] Loss D: 0.0014, Loss G: 9.0582\n",
      "Epoch [5/10] Batch [500/724] Loss D: 0.0014, Loss G: 9.5700\n",
      "Epoch [5/10] Batch [600/724] Loss D: 0.0010, Loss G: 10.5320\n",
      "Epoch [5/10] Batch [700/724] Loss D: 0.0006, Loss G: 10.9214\n",
      "Epoch [5/10] Validation - STOI: 0.8983, PESQ: 1.4309, SNR: 7.9514\n",
      "Epoch [5/10] completed.\n",
      "Epoch [6/10] Batch [0/724] Loss D: 0.0004, Loss G: 10.3209\n",
      "Epoch [6/10] Batch [100/724] Loss D: 0.0010, Loss G: 9.6970\n",
      "Epoch [6/10] Batch [200/724] Loss D: 0.4792, Loss G: 8.4230\n",
      "Epoch [6/10] Batch [300/724] Loss D: 0.0020, Loss G: 9.2860\n",
      "Epoch [6/10] Batch [400/724] Loss D: 0.0009, Loss G: 9.8166\n",
      "Epoch [6/10] Batch [500/724] Loss D: 0.0007, Loss G: 9.7751\n",
      "Epoch [6/10] Batch [600/724] Loss D: 0.0004, Loss G: 10.1819\n",
      "Epoch [6/10] Batch [700/724] Loss D: 0.0171, Loss G: 9.4176\n",
      "Epoch [6/10] Validation - STOI: 0.9051, PESQ: 1.1266, SNR: 5.4634\n",
      "Saved model checkpoints for epoch 6.\n",
      "Epoch [6/10] completed.\n",
      "Epoch [7/10] Batch [0/724] Loss D: 0.0038, Loss G: 11.3163\n",
      "Epoch [7/10] Batch [100/724] Loss D: 0.0042, Loss G: 8.2251\n",
      "Epoch [7/10] Batch [200/724] Loss D: 0.0022, Loss G: 9.8824\n",
      "Epoch [7/10] Batch [300/724] Loss D: 0.0008, Loss G: 12.2519\n",
      "Epoch [7/10] Batch [400/724] Loss D: 0.0066, Loss G: 7.6530\n",
      "Epoch [7/10] Batch [500/724] Loss D: 0.0015, Loss G: 8.7833\n",
      "Epoch [7/10] Batch [600/724] Loss D: 0.0010, Loss G: 8.7226\n",
      "Epoch [7/10] Batch [700/724] Loss D: 0.0016, Loss G: 9.2100\n",
      "Epoch [7/10] Validation - STOI: 0.9228, PESQ: 1.4775, SNR: 9.5218\n",
      "Epoch [7/10] completed.\n",
      "Epoch [8/10] Batch [0/724] Loss D: 0.0015, Loss G: 8.3021\n",
      "Epoch [8/10] Batch [100/724] Loss D: 0.0009, Loss G: 9.4785\n",
      "Epoch [8/10] Batch [200/724] Loss D: 0.0010, Loss G: 9.0223\n",
      "Epoch [8/10] Batch [300/724] Loss D: 0.0004, Loss G: 10.2888\n",
      "Epoch [8/10] Batch [400/724] Loss D: 0.0006, Loss G: 9.5899\n",
      "Epoch [8/10] Batch [500/724] Loss D: 0.0007, Loss G: 9.9248\n",
      "Epoch [8/10] Batch [600/724] Loss D: 0.0009, Loss G: 11.0931\n",
      "Epoch [8/10] Batch [700/724] Loss D: 0.2558, Loss G: 6.7691\n",
      "Epoch [8/10] Validation - STOI: 0.8607, PESQ: 1.0894, SNR: 0.1724\n",
      "Saved model checkpoints for epoch 8.\n",
      "Epoch [8/10] completed.\n",
      "Epoch [9/10] Batch [0/724] Loss D: 0.0036, Loss G: 12.8015\n",
      "Epoch [9/10] Batch [100/724] Loss D: 0.0759, Loss G: 5.2800\n",
      "Epoch [9/10] Batch [200/724] Loss D: 0.0021, Loss G: 9.4379\n",
      "Epoch [9/10] Batch [300/724] Loss D: 0.0009, Loss G: 11.9094\n",
      "Epoch [9/10] Batch [400/724] Loss D: 0.0008, Loss G: 9.3215\n",
      "Epoch [9/10] Batch [500/724] Loss D: 0.0007, Loss G: 8.9084\n",
      "Epoch [9/10] Batch [600/724] Loss D: 0.0012, Loss G: 8.6551\n",
      "Epoch [9/10] Batch [700/724] Loss D: 0.0007, Loss G: 9.2198\n",
      "Epoch [9/10] Validation - STOI: 0.9358, PESQ: 1.7080, SNR: 12.1718\n",
      "Epoch [9/10] completed.\n",
      "Epoch [10/10] Batch [0/724] Loss D: 0.0006, Loss G: 10.1273\n",
      "Epoch [10/10] Batch [100/724] Loss D: 0.0008, Loss G: 9.5384\n",
      "Epoch [10/10] Batch [200/724] Loss D: 0.0009, Loss G: 8.6756\n",
      "Epoch [10/10] Batch [300/724] Loss D: 0.0021, Loss G: 10.6258\n",
      "Epoch [10/10] Batch [400/724] Loss D: 0.0014, Loss G: 11.6109\n",
      "Epoch [10/10] Batch [500/724] Loss D: 0.0006, Loss G: 11.7719\n",
      "Epoch [10/10] Batch [600/724] Loss D: 0.0009, Loss G: 9.7144\n",
      "Epoch [10/10] Batch [700/724] Loss D: 0.0025, Loss G: 8.7882\n",
      "Epoch [10/10] Validation - STOI: 0.9267, PESQ: 1.5642, SNR: 10.9479\n",
      "Saved model checkpoints for epoch 10.\n",
      "Epoch [10/10] completed.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    for i, (noisy, clean) in enumerate(train_loader):\n",
    "        noisy = noisy.to(device)  \n",
    "        clean = clean.to(device)  \n",
    "\n",
    "        batch_size_current = noisy.size(0)\n",
    "        \n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        denoised = generator(noisy)\n",
    "\n",
    "        pred_fake = discriminator(denoised)\n",
    "\n",
    "        valid = torch.ones_like(pred_fake).to(device)\n",
    "        fake = torch.zeros_like(pred_fake).to(device)\n",
    "\n",
    "        loss_G_adv = adversarial_loss(pred_fake, valid)\n",
    "\n",
    "        loss_G_l1 = l1_loss(denoised, clean)\n",
    "\n",
    "        loss_G = loss_G_adv + lambda_l1 * loss_G_l1\n",
    "\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        pred_real = discriminator(clean)\n",
    "        loss_D_real = adversarial_loss(pred_real, valid)\n",
    "\n",
    "        pred_fake = discriminator(denoised.detach())\n",
    "        loss_D_fake = adversarial_loss(pred_fake, fake)\n",
    "\n",
    "        loss_D = (loss_D_real + loss_D_fake) / 2\n",
    "\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{i}/{len(train_loader)}] \"\n",
    "                  f\"Loss D: {loss_D.item():.4f}, Loss G: {loss_G.item():.4f}\")\n",
    "\n",
    "    avg_stoi, avg_pesq , avg_snr = evaluate(generator, val_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Validation - STOI: {avg_stoi:.4f}, PESQ: {avg_pesq:.4f}, SNR: {avg_snr:.4f}\")\n",
    "   \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint_dir = 'checkpoints'\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        torch.save(generator.state_dict(), os.path.join(checkpoint_dir, f\"generator_epoch_{epoch+1}.pth\"))\n",
    "        torch.save(discriminator.state_dict(), os.path.join(checkpoint_dir, f\"discriminator_epoch_{epoch+1}.pth\"))\n",
    "        print(f\"Saved model checkpoints for epoch {epoch+1}.\")\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2555204,
     "sourceId": 4340107,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4870.495946,
   "end_time": "2024-11-25T16:59:40.468485",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-25T15:38:29.972539",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
